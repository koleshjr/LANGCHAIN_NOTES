""" There is a langchain UI to do all this: find it"""


import os
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv())

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import CSVLoader
from langchain.vectorstores import DocArrayInMemorySearch
from langchain.indexes import VectorstoreIndexCreator

file = "Data\diani.csv"
loader = CSVLoader(file_path = file)
data = loader.load()

index = VectorstoreIndexCreator(
    vectorstore_cls= DocArrayInMemorySearch
).from_loaders([loader])

llm = ChatOpenAI(temperature=0.0)
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever = index.vectorstore.as_retriever(),
    verbose = True,
    chain_type_kwargs= {
        "document_separator":"<<<<>>>>"
    }
)
#creating new examples for evaluation using an LLM
from langchain.evaluation.qa import QAGenerateChain
example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI())

new_examples = example_gen_chain.apply_and_parse(
    [{"doc": t} for t in data[:5]]
)
print("<<<<<<<--------------- examples generated by the llm for eval --------->>>>>>>>")
print(new_examples)

import langchain
langchain.debug = True

print("<<<<<<<<-----------------Example qa run on a single query(with debug set to true--------------------------->>>>>>>")
print(qa.run(new_examples[0]["query"]))

print("<<<<<<<<-----------------Example qa run on a multiple queries--------------------------->>>>>>>")
from langchain.evaluation.qa import QAEvalChain
langchain.debug = False
predictions = qa.apply(new_examples)

llm = ChatOpenAI(temperature=0)
eval_chain = QAEvalChain.from_llm(llm)

graded_outputs = eval_chain.evaluate(new_examples, predictions)
for i, eg in enumerate(new_examples[3:]):
    print(f"Example {i}:")
    print("Question: " + predictions[i]['query'])
    print("Real Answer: " + predictions[i]['answer'])
    print("Predicted Answer: "+ predictions[i]['result'])
    print("predicted Grade: " + graded_outputs[i]['text'])
    print()


